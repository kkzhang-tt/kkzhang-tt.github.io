---

layout: post
title: "DDIA: 数据复制（三）"
category: "ddia"
author: "kkzhang"
image: ddia/ddia_7_1.png
---

# 无主节点复制

单主节点 & 多主节点复制中，客户端先向主节点发送写请求，然后系统将写请求复制到其他副本。对于无主节点复制，选择放弃主节点，允许任何副本直接接收来自客户端的请求。

> 亚马逊的 Dynamo 系统是无主复制的典型

在有些无主节点系统中，客户端直接将写请求发送到多副本；而在另一些系统中，由一个协调者节点代表客户端进行写入。

## 节点失效时写入数据库

对于主从复制模型，主节点失效时，如果想继续处理写操作，那么需要先进行节点切换；对于无主节点模型，则不需要节点切换。

![]({{site.baseurl}}/images/ddia/ddia_7_1.png)

1. 客户端将**写请求并行发送给三个副本**，有两个副本可以正常处理请求，而失效的副本无法处理。如果两个正常工作的节点可以成功确认写操作，那么客户端收到两个确认之后，即可认为写入成功；完全忽律其中一个副本无法写入的情况。
2. 失效的节点重新工作后，在其失效期间的数据尚未同步，此时用户可能会访问到过期的数据。因此，当客户端读取数据时，将**读请求并行发送给三个副本**；多个副本的响应结果可能不同，可以通过版本号等技术确定哪个值更新。

### 读修复 & 反熵

复制模型应该确保所有数据最终复制到所有副本上。为了保证失效节点重新追上错过的写请求，经常使用两种机制：

1. **读修复**

   当客户端并行读取多个副本时，可以检测到过期的返回值；因而可以**将新值写入该副本**，这种方法适合频繁读取的场景。

2. **反熵过程**

   通过一些后台进程不断查找副本之间的数据差异，**将任何缺少的数据从一个副本复制到另一副本**。

   > 与基于主节点复制的复制日志不同，反熵过程并不保证数据以特定的顺序复制写入，并且会引入明显的滞后。

有的系统可能没有实现反熵过程，那么当缺少反熵过程时，由于读修复只在发生读取时才能执行修复，那么对于一些很少访问的数据来说，有可能在某些副本中已经丢失而无法被检测到，从而降低写的持久性。

### 读写 quorum

一般情况下，**如果有 n 个副本，写入需要 w 个节点确认，读取至少查询 r 个节点，则只要 w + r > n，那么读取的节点中一定会包含最新值**。在之前的例子中，n = 3, w = 2, r = 2；满足上述这些 r, w 的读写操作称之为*法定票数读（仲裁读）或法定票数写（仲裁写*）。

> r & w 是用于判断读写是否有效的最低票数；通常 w = r =(n+1)/2

仲裁条件 w + r > n 定义了系统可容忍的失效节点书：

- w < n，如果一个节点不可用，仍然可以处理写入
- r < n，如果一个节点不可用，仍然可以处理读取
- 假定 n = 3, w = 2, r = 2，则可以容忍一个节点不可用
- 假定 n = 5, w = 3, r = 3，则可以容忍两个不可用的节点

> 通常，读取与写入总是并行发送到所有的 n 个副本；参数 w & r 只是决定要等待的节点数，即有多少个节点需要返回结果，才能判断出结果的正确性

![]({{site.baseurl}}/images/ddia/ddia_7_2.png)

如果可用节点数小于 w 或者 r，那么写入或者读取就会返回错误。

## Quorum 一致性的局限性

w + r > n 使客户端能够获取最新值是因为，成功写入的节点集合和读取的节点集合必然有重合，这样读取的节点中至少又一个具有最新值。

通常会假定 w & r 为大多数（多于 n / 2）节点，此时能够容忍 n / 2 个节点故障；但是 quorum 并不一定非得是多数，读写的节点集中又一个重叠的节点才是关键。

如果 w + r ≤ n，此时可以获得更低的延迟和更高的可用性；但是读取请求中可能恰好没有包含最新值的节点，客户端就会获取一个过期的旧值。

不过，即使满足 w + r > n，也可能存在返回旧值的边界条件：

- 如果采用了 sloppy quorum，那么*写操作的 w 个节点和读取的 r 个节点可能完全不同*，因此无法保证读写请求一定存在重叠的节点
- 如果写操作与读操作同时发生，写操作肯跟仅在一部分副本上完成，此时读取返回是旧值还是新值存在不确定性
- 如果某些副本写入成功，其他副本写入失败，且成功副本数 < n，哪些成功的副本不会进行回滚；那么后续的读仍可能返回新值
- 两个写操作同时发生，无法明确先后顺序；这种情况下采用了 Last Write Win 方案，此时可能会把新值给抛弃

虽然 Quorum 设计上似乎可以保证读取最新值，但是实际情况往往更加复杂。

### 宽松的 Quorum & 数据回传

Quorum 可以容忍某些节点故障或者变慢，并且只需要等待 w / r 个节点的响应（而不是 n 个节点），所以对于高可用与低延迟的场景还是很有帮助。

考虑网络中断的情况，假设某个客户端无法维持满足最低的 w & r 所要求的连接数，从而无法正常工作；但是能够连接到其他数据库节点，只不过这些节点不再 n 个节点的集合中。这种情况下，我们可以采用宽松的 **Quorum 机制：写入和读取仍然需要 w & r 个成功的响应，但是包含了哪些并不在之前指定的 n 个节点集合中**。*一旦网络问题得到解决，临时节点需要把接收到的写入全部发送到原始节点上，即数据回传*。

Sloppy Quorum 对于提高写入可用性比较有帮助：**只要任何 w 个节点可用，数据库就可以接受新的写入；然而这会意味着即使满足w + r > n，也不能保证在读取某个记录时，一定能够读到最新值**。

> 因为新值可能被临时写入 n 之外的那些节点且尚未回传过来

## 并发写冲突

Dynamo 风格的数据库允许多个客户端同时对相同的记录进行并发写，即使采用严格的 quorum 机制也会发生写冲突；同时，读时修复 & 数据回传也会导致并发冲突。

![]({{site.baseurl}}/images/ddia/ddia_7_3.png)

> 并发写时缺乏顺序保证

### 最后写入者获胜

有一种可以实现最终收敛的方式：**每个副本总是保存最新值，允许覆盖并丢弃旧值**。

> 丢弃并发写入

为了确定写入的顺序，可以强制对并发写入进行排序：*可以为每个写请求附加一个时间戳，然后选择最大的时间戳，丢弃较小的时间戳写入（Last Write Win）*。LWW 可以实现最终的收敛，但是是以牺牲数据持久化为代价（多个并发写只会保留一个，其他的都会被丢弃）。

如果某些场景可以接受覆盖写，那么可以使用 LWW；如果覆盖，丢失数据不可接受，那么 LWW 不是解决冲突的好选择。

### Happen-Before 关系和并发

对于两个动作 A，B：

- 如果 B 知道 A，或者依赖 A，或者以某种方式在 A 基础上构建，则称 A 在 B 之前发生
- 如果两个操作都不在另一个之前发生，或者两者都不知道对方，那么 A & B 是并发的

因此对于 A，B 两个动作，一共有三种可能性：A 在 B 之前发生，B 在 A 之前发生，A & B并发。

通常如果两个动作同时分发生，那么可以称为并发；但是由于分布式系统时钟的问题，很难严格确定发生的时间是否相同。因此，为了更好地定义并发，我们并不依赖确切的发生时间，即不管物理的时机如何，**如果两个操作都不需要意识到对方，即可称其为并发操作**。

下面是两个客户端同时向购物车添加商品：

![]({{site.baseurl}}/images/ddia/ddia_7_4.png)

> 并发操作购物车说明事件的因果关系

![]({{site.baseurl}}/images/ddia/ddia_7_5.png)

> 对应的时序图

### 合并同时写入的值

如果使用 LWW 的话，会有部分数据丢失；为了防止数据丢失，可以将并发的值进行合并。

### 版本矢量

还有一种方法：为每个副本和每个主键均定义一个版本号，*每个副本在处理写入时增加自己的版本号，并跟踪从其他副本看到的版本号*。通过这些信息来确定要覆盖哪些值，保留哪些并发值。